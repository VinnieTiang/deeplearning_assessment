{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86bdf6dd",
      "metadata": {
        "id": "86bdf6dd"
      },
      "source": [
        "Vinnie Tiang Wen Ying 22004876\n",
        "# Section C: Deep Learning Mini Implementation Notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IOu8mLIk2Szb",
      "metadata": {
        "id": "IOu8mLIk2Szb"
      },
      "source": [
        "# Deep Learning Concept: Backpropagation\n",
        "\n",
        "Backpropagation is the core learning algorithm used to train neural networks. Throughout the model training process, backpropagation will calculate how much each weight and bias in the network contributes to the total error (loss), then adjusts them to reduce that error, in order to get better prediction output. The loss here is the difference between predicted output and the actual output.\n",
        "\n",
        "The backpropagation process\n",
        "1. **Forward Pass**: Make a prediction using current weights and biases.\n",
        "2. **Loss Calculation**: Measure the difference between predicted and actual output.\n",
        "3. **Backward Pass**: Compute gradients of the loss with respect to each weight and bias.\n",
        "4. **Gradient Descent**: Update the weight and bias to reduce the loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c14e16",
      "metadata": {
        "id": "91c14e16"
      },
      "source": [
        "## Concept Illustration of How a Neural Network **Learns a Simple Formula** using Backpropagation\n",
        "\n",
        "In this implementation, I will demo using a very simple network architecture (a single-layer perceptron with no hidden layers)\n",
        "\n",
        "*   Input layer: 1 input value (ùë•)\n",
        "*   Neuron: 1 neuron with 1 weight (ùë§) and 1 bias (ùëè)\n",
        "*   Output layer: 1 output value (ùë¶ÃÇ) (prediction)\n",
        "\n",
        "\n",
        "\n",
        "I will demo on how this single neuron use backpropagation to learn a linear function\n",
        "\n",
        "**y = 3x ‚àí 2**\n",
        "\n",
        "where given the input ùë•, the model should be able to predict the output 3ùë•-2.\n",
        "\n",
        "Even the simplest neural network can learn useful relationships when trained properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c66gj7Rl925A",
      "metadata": {
        "id": "c66gj7Rl925A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xRNDTbzNC-CT",
      "metadata": {
        "id": "xRNDTbzNC-CT"
      },
      "source": [
        "## Initialize Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "mtTRjpVlAWTt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtTRjpVlAWTt",
        "outputId": "2be03170-ddea-4551-9a98-034e824d93f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial weight: 0.4967\n",
            "Initial bias: -0.1383\n",
            "\n",
            "\n",
            "What we want to learn now:\n",
            "Given input: x = 2.0\n",
            "Target output: y = 4.00\n"
          ]
        }
      ],
      "source": [
        "# Dataset: learning y = 3x - 2\n",
        "X = 2.0                 # Single input for now, x = 2\n",
        "y_target = 3 * X - 2    # Target: y = 4\n",
        "\n",
        "# Initialize weight, bias, learning rate\n",
        "np.random.seed(42)\n",
        "w = np.random.randn() # weight\n",
        "b = np.random.randn() # bias\n",
        "lr = 0.01   # Learning rate\n",
        "\n",
        "print(f\"Initial weight: {w:.4f}\")\n",
        "print(f\"Initial bias: {b:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"What we want to learn now:\")\n",
        "print(f\"Given input: x = {X}\")\n",
        "print(f\"Target output: y = {y_target:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5NBUm1UuAc0Y",
      "metadata": {
        "id": "5NBUm1UuAc0Y"
      },
      "source": [
        "## Step 1 - Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "xZjatetPBSC_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZjatetPBSC_",
        "outputId": "f95d2c06-5cbc-452b-b674-1e31b0863f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted y: 0.8552\n",
            "Target y: 4.0000\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Forward pass to compute prediction\n",
        "y_pred = w * X + b\n",
        "\n",
        "print(f\"Predicted y: {y_pred:.4f}\")\n",
        "print(f\"Target y: {y_target:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Amyt1CUHBqFb",
      "metadata": {
        "id": "Amyt1CUHBqFb"
      },
      "source": [
        "## Step 2 - Loss Calculation (Mean Squared Error)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mwS9Mc1FPgSY",
      "metadata": {
        "id": "mwS9Mc1FPgSY"
      },
      "source": [
        "We use **Mean Squared Error (MSE)** as our loss function.\n",
        "\n",
        "MSE calculate the average squared difference between the predicted output (`≈∑`) and the true output (`y`).  \n",
        "- If predicted output far from the target, loss will be large.\n",
        "- If predicted output close to the target, loss will be small.\n",
        "\n",
        "Our Goal: minimize loss value, so model make better predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1K4DcQbwBpKs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K4DcQbwBpKs",
        "outputId": "2affff63-4db0-4610-ed6a-4f65a7eb49cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 9.8900\n",
            "The lower the loss value, the better the prediction is.\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Compute loss using MSE\n",
        "loss = np.mean((y_target - y_pred) ** 2)\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "print (f\"The lower the loss value, the better the prediction is.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Y566QtpGka3",
      "metadata": {
        "id": "5Y566QtpGka3"
      },
      "source": [
        "## Step 3: Backward Pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sOVf5hPuQjnv",
      "metadata": {
        "id": "sOVf5hPuQjnv"
      },
      "source": [
        "Since the loss is high, we need to improve the model by adjusting the weight `w` and bias `b`.\n",
        "\n",
        "We use backpropagation, which tells us how much each parameter contributed to the error, using calculus (**chain rule**).\n",
        "\n",
        "---\n",
        "\n",
        "We break down the effect of each parameter step by step:\n",
        "\n",
        "First, we calculate the gradient of the loss with respect to the predicted output:\n",
        "```\n",
        "dL/dùë¶ÃÇ = -2(y-ùë¶ÃÇ)\n",
        "#This comes from the derivative of the MSE loss: L=(y-ùë¶ÃÇ)^2\n",
        "\n",
        "```\n",
        "Then, we compute how the predicted output changes with respect to the weight:\n",
        "```\n",
        "dùë¶ÃÇ/dw = x\n",
        "```\n",
        "Using chain rule, we combine and get:\n",
        "\n",
        "```\n",
        "dL/dw = dL/dùë¶ÃÇ √ó dùë¶ÃÇ/dw\n",
        "= -2(y-ùë¶ÃÇ) √ó x\n",
        "```\n",
        "\n",
        "Similarly, the predicted output changes with respect to the bias:\n",
        "```\n",
        "dùë¶ÃÇ/db = 1\n",
        "```\n",
        "So the gradient of the loss with respect to the bias is:\n",
        "\n",
        "\n",
        "```\n",
        "dL/db = dL/dùë¶ÃÇ √ó dùë¶ÃÇ/db  \n",
        "      = -2(y - ùë¶ÃÇ) √ó 1  \n",
        "      = -2(y - ùë¶ÃÇ)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "GuV2OfC0GnfO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuV2OfC0GnfO",
        "outputId": "c8b19c7c-12a8-4100-aa1e-258e4ce41ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient with respect to weight (dw): -12.5793\n",
            "Gradient with respect to bias   (db): -6.2897\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Backpropagation (Compute gradients)\n",
        "dL_dy = -2 * (y_target - y_pred)  # Gradient of loss with respect to prediction, dL/dyÃÇ from MSE\n",
        "dw = np.sum(dL_dy * X)          # dL/dw using Chain Rule\n",
        "db = np.sum(dL_dy)              # dL/db using Chain Rule\n",
        "\n",
        "print(f\"Gradient with respect to weight (dw): {dw:.4f}\")\n",
        "print(f\"Gradient with respect to bias   (db): {db:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdZqoU0zGt6Z",
      "metadata": {
        "id": "fdZqoU0zGt6Z"
      },
      "source": [
        "These gradients tell us how much each parameter affects the loss.\n",
        "In gradient descent, we have to always update the parameter in the opposite direction of the gradient.\n",
        "- If the gradient is positive, means increasing the parameter increases the loss. So we reduce the parameter.\n",
        "- If the gradient is negative, means increasing the parameter reduces the loss. So we increase the parameter.\n",
        "\n",
        "This helps us to move towards lower loss values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D_5-ln7uG0KH",
      "metadata": {
        "id": "D_5-ln7uG0KH"
      },
      "source": [
        "## Step 4: Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GeVSUI6VUdOI",
      "metadata": {
        "id": "GeVSUI6VUdOI"
      },
      "source": [
        "In gradient descent, we always move opposite to the gradient to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RfzCZhq8BEG6",
      "metadata": {
        "id": "RfzCZhq8BEG6"
      },
      "source": [
        "One key thing to mention here is the **learning rate (lr)**.\n",
        "\n",
        "It controls how big a step the model takes when updating the weights and bias during training, and thus controls how fast or slow the model will learn.\n",
        "\n",
        "- If learning rate is too large, the model might overshoot and fail to converge.\n",
        "- If learning rate too small, training will be very slow and might get stuck in local minima.\n",
        "\n",
        "In step 1, I had set lr = 0.01.\n",
        "\n",
        "This means we update the weight and bias using 1% of the gradient value at every step. This balances the learning as it will not make the updates too aggressive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Br3Ui-VDGwgp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3Ui-VDGwgp",
        "outputId": "0e7c55c4-e0e2-44c5-951e-26fe119ef2bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated weight: 0.6225\n",
            "Updated bias:   -0.0754\n",
            "\n",
            "Comparison Before and After the First Backpropagation Process\n",
            "Weight:     0.4967 ‚Üí 0.6225\n",
            "Bias:       -0.1383 ‚Üí -0.0754\n",
            "Prediction: 0.8552 ‚Üí 1.1696, (we aim to get closer to target y = 4.0000)\n",
            "Loss:       9.8900 ‚Üí 8.0109, (loss is reducing)\n"
          ]
        }
      ],
      "source": [
        "## Step 4: Update Parameters using Gradient Descent\n",
        "\n",
        "# Before updating\n",
        "w_before = w\n",
        "b_before = b\n",
        "y_pred_before = y_pred\n",
        "loss_before = np.mean((y_target - y_pred_before) ** 2)\n",
        "\n",
        "# In gradient descent, we always move opposite to the gradient to minimize the loss\n",
        "w -= lr * dw\n",
        "b -= lr * db\n",
        "\n",
        "# After updating\n",
        "y_pred_after = w * X + b\n",
        "loss_after = np.mean((y_target - y_pred_after) ** 2)\n",
        "\n",
        "# Print updated values\n",
        "print(f\"\\nUpdated weight: {w:.4f}\")\n",
        "print(f\"Updated bias:   {b:.4f}\")\n",
        "\n",
        "# Compare before and after\n",
        "print(\"\\nComparison Before and After the First Backpropagation Process\")\n",
        "print(f\"Weight:     {w_before:.4f} ‚Üí {w:.4f}\")\n",
        "print(f\"Bias:       {b_before:.4f} ‚Üí {b:.4f}\")\n",
        "print(f\"Prediction: {y_pred_before:.4f} ‚Üí {y_pred_after:.4f}, (we aim to get closer to target y = {y_target:.4f})\")\n",
        "print(f\"Loss:       {loss_before:.4f} ‚Üí {loss_after:.4f}, (loss is reducing)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dDuxEO1DG72D",
      "metadata": {
        "id": "dDuxEO1DG72D"
      },
      "source": [
        "The model learns by adjusting its parameters to reduce the error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r4vTLoZsG-lN",
      "metadata": {
        "id": "r4vTLoZsG-lN"
      },
      "source": [
        "## Let's see how the Model learns in Actual Training Loop (Multiple Epochs)\n",
        "Every epoch:\n",
        "1. Takes input x\n",
        "2. Computes ≈∑ = w¬∑x + b (forward pass)\n",
        "3. Compares with true y using MSE loss\n",
        "4. Calculates gradients (backpropagation)\n",
        "5. Updates w and b using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "11SScO56HZ1N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11SScO56HZ1N",
        "outputId": "5911ee16-3ebc-40b2-a103-a7cbcd6cf25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 | Loss: 22.4229 | w: 0.7226 | b: -0.0754\n",
            "Epoch 100 | Loss: 0.6024 | w: 2.5325 | b: -0.6673\n",
            "Epoch 200 | Loss: 0.1819 | w: 2.7431 | b: -1.2676\n",
            "Epoch 300 | Loss: 0.0549 | w: 2.8588 | b: -1.5975\n",
            "Epoch 400 | Loss: 0.0166 | w: 2.9224 | b: -1.7788\n",
            "Epoch 500 | Loss: 0.0050 | w: 2.9574 | b: -1.8785\n",
            "Epoch 599 | Loss: 0.0015 | w: 2.9764 | b: -1.9328\n"
          ]
        }
      ],
      "source": [
        "# Prepare full training dataset\n",
        "X = np.array([0, 1, 2, 3, 4], dtype=float)\n",
        "y_target = 3 * X - 2 # Target output: y = 3x - 2\n",
        "\n",
        "# Reinitialize weight and bias\n",
        "np.random.seed(42)\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(600):\n",
        "    # Step 1: Forward pass\n",
        "    y_pred = w * X + b\n",
        "\n",
        "    # Step 2: Loss calculation\n",
        "    loss = np.mean((y_target - y_pred) ** 2)\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Step 3: Backpropagation\n",
        "    dL_dy = -2 * (y_target - y_pred) / len(X)\n",
        "    dw = np.sum(dL_dy * X)\n",
        "    db = np.sum(dL_dy)\n",
        "\n",
        "    # Step 4: Gradient Descent\n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "    # Print every 100 epochs\n",
        "    if epoch % 100 == 0 or epoch == 599:\n",
        "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | w: {w:.4f} | b: {b:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sCJwnRKUC89u",
      "metadata": {
        "id": "sCJwnRKUC89u"
      },
      "source": [
        "- We can see at Epoch 0, it is a random start, so we get bad loss.\n",
        "\n",
        "- Throughout the Epoch to 599th Epoch, we get decreasing loss value, the weight and bias become closer and closer to the ideal w=3, b=-2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPN5275kXisD",
      "metadata": {
        "id": "wPN5275kXisD"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "P4SXUez1iv1_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "P4SXUez1iv1_",
        "outputId": "814bd4b0-f48f-4655-ee5e-b8a51e32260a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss Over Time\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TPLYePH4iwy1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPLYePH4iwy1",
        "outputId": "a2ef60b6-2ca0-44cf-ca94-28f6a4a6b08d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Target weight : 3.0 | Learned weight: w = 2.9764\n",
            "Target bias : -2.0 | Learned bias:   b = -1.9328\n",
            "\n",
            "\n",
            "Compare with the true target function y = 3x - 2\n",
            "\n",
            "Given x = 2\n",
            "True y = 4.0000\n",
            "Predicted y = 4.0201\n"
          ]
        }
      ],
      "source": [
        "# Final learned weight and bias\n",
        "print(f\"\\nTarget weight : 3.0 | Learned weight: w = {w:.4f}\")\n",
        "print(f\"Target bias : -2.0 | Learned bias:   b = {b:.4f}\")\n",
        "\n",
        "print(f\"\\n\\nCompare with the true target function y = 3x - 2\")\n",
        "x_check = 2\n",
        "y_target_check = 3 * x_check - 2\n",
        "y_pred_check = w * x_check + b\n",
        "\n",
        "print(f\"\\nGiven x = {x_check}\")\n",
        "print(f\"True y = {y_target_check:.4f}\")\n",
        "print(f\"Predicted y = {y_pred_check:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rDtSXx5YqJEY",
      "metadata": {
        "id": "rDtSXx5YqJEY"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this implementation, I had shown the concept of backpropagation using a mini neural network to learn the function **y = 3x ‚àí 2**.\n",
        "\n",
        "We can see that even with just one weight and one bias, the network was able to reduce the error and produce accurate predictions, showing the power of backpropagation in its simplest form.\n",
        "\n",
        "Through the training loop, the model:\n",
        "- Performed a **forward pass** to make predictions\n",
        "- Calculated **loss** using Mean Squared Error (MSE)\n",
        "- Applied **backpropagation** to compute gradients\n",
        "- Used **gradient descent** to update the weight and bias\n",
        "\n",
        "At the end of training, the learned weight and bias closely matched to the target values of **w ‚âà 3 and b ‚âà -2**. This shows how backpropagation enables learning in neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hGrTUJfZ8Ogu",
      "metadata": {
        "id": "hGrTUJfZ8Ogu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7z-n-tt8MyW",
      "metadata": {
        "id": "N7z-n-tt8MyW"
      },
      "source": [
        "### Real-World Perspective\n",
        "\n",
        "In real-world deep learning applications, such as:\n",
        "- Image recognition (classifying butterflies)\n",
        "- Natural language processing (chatbots/translation)\n",
        "- Medical diagnosis (detect tumors from scans)\n",
        "\n",
        "the same concept apply but at a larger scale. Those models will have millions or billions of parameters, structured in complex architectures. For example,\n",
        "- Deep feedforward networks\n",
        "- Convolutional Neural Networks (CNNs) for image data\n",
        "- Recurrent Neural Networks (RNNs) or Transformers for sequence data\n",
        "\n",
        "But still, at the core, every single weight in these large models is updated using the backpropagation concept, just like in this tiny one-neuron example.\n",
        "\n",
        "This simple exercise shows that, if a single neuron can learn a linear rule using backpropagation, so then a deep network can learn complex patterns, by stacking and training many neurons together."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
